{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch_MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kF0hn4MwWxe0",
        "colab_type": "text"
      },
      "source": [
        "## **MNIST Classifier using Pytorch**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EE5yer7yXHXN",
        "colab_type": "text"
      },
      "source": [
        "<p>In this notebook, we will use pytorch to classify the handwritten digits in the classic MNIST dataset. We will use a convolutional network to enhance to accuracy of our classifier than a regular neural network.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVoeU5HMYTWB",
        "colab_type": "text"
      },
      "source": [
        "<p>Let's start by loading libraries.</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kwu5Af9AWqyk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch \n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PDY4UfBhQmy",
        "colab_type": "text"
      },
      "source": [
        "Next step is to load and the data into train and testsets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHiOwub-gJUM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transform = transforms.Compose([\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size = 64, shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size = 64, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVnN7Qt5lxlc",
        "colab_type": "text"
      },
      "source": [
        "Analysing the train set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlpnvdqrlMNM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "50172596-6f23-4982-b631-3763cb1ebdb5"
      },
      "source": [
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "images.shape, labels.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([64, 1, 28, 28]), torch.Size([64]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXUH9b3pmn1e",
        "colab_type": "text"
      },
      "source": [
        "So one train data batch is a  tensor of shape: <code>(torch.Size([64, 1, 28, 28])</code> This means we have 64 examples of 28x28 gray scale images. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PG1nucj_nAuc",
        "colab_type": "text"
      },
      "source": [
        "Now lets see some of them randomly using matplotlib.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_-2zIZvl35h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Gw25M5GnJTB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "outputId": "86faf292-5498-49ad-a0d3-d7a563a8b49d"
      },
      "source": [
        "plt.figure(figsize = (14, 10))\n",
        "for i in range(6):\n",
        "  plt.subplot(2,3,i+1)\n",
        "  plt.imshow(images[i][0], cmap='gray', interpolation='none')\n",
        "  plt.title(\"Actual Label: {}\".format(labels[i]))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAy8AAAI9CAYAAAAzREcUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOzdebRU5ZX38d8WHMGBWVSExDgRe4EGMW8LiknaAYOC7QKHALam0dgabWcRDU7d6jKiRsVAi+BA1BhQ2jYqrTi9rSagtIKzBmQm4ASRRPHu9486vrmSe59Tt865VfVUfT9r3cW99Tv3nE1pbWrXqTqPubsAAAAAoNptVukCAAAAAKAYDC8AAAAAosDwAgAAACAKDC8AAAAAosDwAgAAACAKDC8AAAAAosDwgq8xs/Fmdk/O++xlZm5mbcv5uwAqi34CIA/0EjTG8FJlzOxpM/vIzLYscvuTzOz51q4rOdYgM1tajmOVKmkojyb34Uozu4XmgnpFPymdmW1pZneY2WIzW2dm883siErXBVQCvSQ7MzvOzN4wsz+Z2XtmNrDSNcWK4aWKmFkvSQMluaSjKlpMvG6TtFpSd0l9JR0s6fSKVgRUAP0ks7aSlqjQQ7aXNE7SA8n9CtQNekl2ZvYPkq6V9E+StpV0kKT3K1pUxBheqssoSS9KmippdOPAzHqY2Qwz+6OZrU3OKOwt6XZJ/8fM1pvZx8m2T5vZjxv97tdeATGzm8xsiZl9ambz8pj+zexIM3sl2ecSMxvfxGYnm9lyM1thZuc1+t3NzOyi5JWItWb2gJl1LLGUb0h6wN3/7O4rJT0m6dsl7guIGf0kQz9x9z+5+3h3X+TuDe7+iKQ/SPpO6X8zIEr0kuzPTS6XdIW7v5j0k2XuvqzEfdU9hpfqMkrSvcnXYWbWTZLMrI2kRyQtltRL0s6S7nP3NySdJukFd2/v7jsUeZzfq3BWoqOk6ZJ+bWZbZaz9T0n9O0g6UtJPzGzoJtscIml3SYdKutDMfpDcfqakoSq8wrmTpI8k3drUQZJG8kigjhslHWdm25jZzpKOUGGAAeoN/SR7P2m8bTdJe0ha2LK/ChA9ekmGXpLcT/0kdTGzd81saTLkbZ3tr1a/GF6qhJkNkNRThbMG8yS9J+mEJO6vwgPn/OTVwD+7e8nvJXX3e9x9rbtvdPefS9pS0p5Z6nf3p939teQVhVcl/UqFB3xjlyf1vybpTknHJ7efJukSd1/q7n+RNF7SsdbEZ1Xc/Rp3/2GglGdVONPyqaSlkuZKeijL3w2IDf0kt34iSTKzzVV44jbN3d8s/W8GxIVekksv6SZpc0nHqvD2u76S9lXhragoAcNL9Rgt6Ql3X5P8PF1/PT3bQ9Jid9+Yx4HM7DwrfGjsk+R07vaSOmfc5wFmNic5dfyJCg/6Tfe5pNH3i1VoelKhMc40s4+Tet6Q9KUKD/iW1LCZCmdZZkhqlxy/gwrvMwXqCf0kYz9pVMtmku6W9LmkM0rZBxAxekn2XrIh+fMX7r4iuS9vkDS4hftBgqswVYHk1OFwSW3MbGVy85aSdjCzPio8sHY1s7ZNNAlvYpd/krRNo593bHSsgZIukPR9SQvdvcHMPpJkGf8a0yXdIukId/+zmd2ov20QPSR99arlrpKWJ98vkXSyu//fTXdqLftwbMdkv7ckr5L8xczulHSVCn9noObRT3LrJzIzk3SHCk9WBrv7Fy35fSBm9JJ8eom7f2SFq6E1vk+aun9QJM68VIehKkzzvVU4ndhX0t6SnlPhvZq/k7RC0jVm1s7MtjKzA5PfXSVpFzPbotH+5ks6Jvncx7ckndIo21bSRkl/lNTWzC6TtF1Lik2O3/jLkv1+mDSH/vrraeXGLk1q+rYKV9y4P7n9dklXm1nPZP9dzOzoltQkScmrGX9Q4T2tbc1sBxVeIXq1pfsCIkY/yaGfJCaqcN8NcfcNaRsDNYZekl8vuVPSmWbW1cw6SPpXFT4vhBIwvFSH0ZLudPcP3H3lV18qvFpwogqvPAyR9C1JH6jwWY4Rye8+pcIHSFea2VendSeo8BaHVZKmqfBe7a88rsJbq95W4fTon/X1U6ZpdlbhFGjjr91UuBzxFWa2TtJlkh5o4nefkfSupCclXe/uTyS33yRplqQnkt9/UdIBTR3czMaa2W8D9R0j6XAVGuC7kr5QoUkA9YJ+kkM/SZ6wnKrCE7aVVrhq0nozO7EFfz8gZvSS/J6bXKnCBQneVuHtZ69IuroFfz80Yu6cuQIAAABQ/TjzAgAAACAKDC8AAAAAosDwAgAAACAKDC8AAAAAosDwAgAAACAKmRapNLPDVbiUXBtJ/+Hu16Rsz6XNgOqzxt27VLoI+gkQP3fPuqhgLlrST+glQFVq9rlJyWdezKyNpFslHaHCAkbHm1nvUvcHoGIWV7oA+gmAvNBPgJrQ7HOTLG8b6y/pXXd/390/l3SfpFJXHgVQ3+gnAPJCPwFqWJbhZWd9ffXTpcltANBS9BMAeaGfADUs02deimFmYySNae3jAKh99BMAeaCXAPHKMrwsk9Sj0c+7JLd9jbtPkjRJ4kNxAJpFPwGQl9R+Qi8B4pXlbWO/l7S7mX3DzLaQdJykWfmUBaDO0E8A5IV+AtSwks+8uPtGMztD0uMqXIpwirsvzK0yAHWDfgIgL/QToLaZe/nOlnJqFqhK89y9X6WLaCn6CVB9qmWdl5aglwBVqdnnJlneNgYAAAAAZcPwAgAAACAKDC8AAAAAosDwAgAAACAKDC8AAAAAosDwAgAAACAKDC8AAAAAosDwAgAAACAKDC8AAAAAosDwAgAAACAKDC8AAAAAosDwAgAAACAKDC8AAAAAosDwAgAAACAKDC8AAAAAotC20gUAAAAAlbbXXnulbnP22WcH82HDhgVzMwvm/fr1C+YffPBBMK8HnHkBAAAAEAWGFwAAAABRYHgBAAAAEAWGFwAAAABRYHgBAAAAEAWGFwAAAABRYHgBAAAAEAXWeUGT0q5DfsUVVwTzcePGBXN3z1zD7Nmzg/mxxx4bzD/99NPUGgAAQH24++67U7fZb7/9gnna85u05zadO3cO5qzzwpkXAAAAAJFgeAEAAAAQBYYXAAAAAFFgeAEAAAAQBYYXAAAAAFFgeAEAAAAQBYYXAAAAAFFgnRc0afjw4cF87NixwbyhoSFzDWnXSv/+978fzK+77rpgftppp7W4JiBPadf732abbcpUSTaff/55MP/iiy/KVAkANK9nz57BfNddd03dR1rfTvPyyy8Hc9ZxSZdpeDGzRZLWSfpS0kZ375dHUQDqD/0EQF7oJ0DtyuPMyyHuviaH/QAA/QRAXugnQA3iMy8AAAAAopB1eHFJT5jZPDMbk0dBAOoW/QRAXugnQI3K+raxAe6+zMy6SpptZm+6+7ONN0iaBo0DQBr6CYC8BPsJvQSIV6YzL+6+LPlztaSZkvo3sc0kd+/Hh+UAhNBPAOQlrZ/QS4B4lTy8mFk7M9v2q+8lHSppQV6FAagf9BMAeaGfALUty9vGukmamVzvuq2k6e7+WC5VoeLS1nHJas2a9AvAtG/fPphvtdVWwTxtHZjtttsumH/66afBHLmqyX6y/fbbB/NrrrkmmJ966ql5ltNqXnnllWB+3nnnBfM5c+bkWQ5Qk/0E2T366KPBvFOnTqn7ePbZZ1O3CTn22GODeTHPj+pdycOLu78vqU+OtQCoU/QTAHmhnwC1jUslAwAAAIgCwwsAAACAKDC8AAAAAIgCwwsAAACAKDC8AAAAAIgCwwsAAACAKGRZ5wUR69MnfBXJb37zm8H8o48+Cua33XZbMJ88eXIwl6SuXbsG83PPPTeYjxgxIpi3a9cumLPOC7KaMWNGMD/kkEPKVEnr2nfffYP5ww8/HMxnzpwZzNPux7T9A6gNaf9u33XXXcF87733DuZpvUZKX6cFrY8zLwAAAACiwPACAAAAIAoMLwAAAACiwPACAAAAIAoMLwAAAACiwPACAAAAIAoMLwAAAACiwPACAAAAIArm7uU7mFn5Doaggw8+OJg/9dRTwTxtEcozzzyzxTW1VNpiVQMGDAjmixYtCuZr165NrWHNmjWp20Rgnrv3q3QRLRVDP/nggw+C+S677JL5GO+9914wf/PNNzPtv3v37qnb7LfffpmOkSbt36mTTz45dR/Tpk3LqxwEuLtVuoaWiqGXoOBHP/pRMJ86dWowHz16dDBPWzBXkj777LPUbZCLZp+bcOYFAAAAQBQYXgAAAABEgeEFAAAAQBQYXgAAAABEgeEFAAAAQBQYXgAAAABEgeEFAAAAQBTaVroAxOnll19u9WPstNNOwXzMmDHB/LTTTgvmXbp0CebFrOHSt2/fYL5ixYrUfaB2fe973wvm48aNC+YXX3xx6jHWr18fzNetW5e6j5Ctt946dZuePXsG87Q1Vvbee+9g3r59+2B+yimnBPNiagDQutLWZpOku+66K5gPGzYsmM+YMSOY33vvvak1oPpx5gUAAABAFBheAAAAAESB4QUAAABAFBheAAAAAESB4QUAAABAFBheAAAAAESB4QUAAABAFFjnpU5t3LgxmDc0NATzrbbaKphvueWWwfyss84K5pJ04oknBvN99tkndR8h7h7M582bl7qPYtaCQf169913g/lJJ51UnkIy2LBhQ+o2b775ZjA/4IADgvmAAQOC+axZs4J5Mb2gR48ewXzJkiWp+wBQuosuuih1m6OPPjqYp63jMmrUqBbVhDilnnkxsylmttrMFjS6raOZzTazd5I/O7RumQBqAf0EQF7oJ0B9KuZtY1MlHb7JbRdJetLdd5f0ZPIzAKSZKvoJgHxMFf0EqDupw4u7Pyvpw01uPlrStOT7aZKG5lwXgBpEPwGQF/oJUJ9K/cB+N3dfkXy/UlK3nOoBUH/oJwDyQj8BalzmD+y7u5tZs598NrMxksZkPQ6A2kc/AZCXUD+hlwDxKvXMyyoz6y5JyZ+rm9vQ3Se5ez9371fisQDUNvoJgLwU1U/oJUC8Sh1eZkkanXw/WtLD+ZQDoA7RTwDkhX4C1DhLW+vCzH4laZCkzpJWSfqZpIckPSBpV0mLJQ13900/NNfUvsIHQ9VYuHBhMO/WLfw24rT1Lfbff/8W15S3xx57LJgfeeSRZaqk4uaV69VH+gmaMmjQoGD+3//938F8s83SX4fr3bt3ME9bqwbFcXcr17Hy6if0knx06dIlmD/zzDOp+1i9utkT75Kk0047LZjzOK4pzT43Sf3Mi7sf30z0/UwlAag79BMAeaGfAPWp1LeNAQAAAEBZMbwAAAAAiALDCwAAAIAoMLwAAAAAiALDCwAAAIAoMLwAAAAAiALDCwAAAIAopK7zgtrUtm34P71ZeJ2xDh06BPNyLELZ0NAQzH/xi18E8wsuuCDPcgCUaNy4ccE8bRHKtIXtJOmTTz5pUU0AWuauu+4K5nvuuWfqPn75y18GcxahhMSZFwAAAACRYHgBAAAAEAWGFwAAAABRYHgBAAAAEAWGFwAAAABRYHgBAAAAEAWGFwAAAABRYJ2XGtSvX7/Ube67775g/o1vfCOvckr2X//1X8E8bW2IV199Nc9yAJRoyJAhwfyQQw7JtP8XXnghdZsVK1ZkOgZQ777zne8E8/322y+Y33TTTanHKGYbgDMvAAAAAKLA8AIAAAAgCgwvAAAAAKLA8AIAAAAgCgwvAAAAAKLA8AIAAAAgCgwvAAAAAKLAOi9VqF27dsH8/PPPD+YXXnhh6jG22GKLFtWUtxtuuCF1myuuuCKYr1u3Lq9yAGTQoUOHYD5hwoRgbmbBfOPGjcH82muvDeYAsnv00UeDeadOnYL5jBkz8iynal1yySXBfOjQoZn2P3ny5NRt0u7rNWvWZKqh0jjzAgAAACAKDC8AAAAAosDwAgAAACAKDC8AAAAAosDwAgAAACAKDC8AAAAAosDwAgAAACAKrPNSAdttt10wHzlyZDC/9NJL8yynIsaOHZu6zRdffFGGSgBkdeuttwbzb37zm5n2f9VVVwXzF198MdP+AUgHHXRQMO/SpUswv/rqq4P5888/3+KaqtGVV14ZzNOe37z11lvBfM899wzmt99+ezCXpLlz5wbzml/nxcymmNlqM1vQ6LbxZrbMzOYnX4Nbt0wAtYB+AiAv9BOgPhXztrGpkg5v4vYJ7t43+QovuwoABVNFPwGQj6minwB1J3V4cfdnJX1YhloA1Dj6CYC80E+A+pTlA/tnmNmryWnbDrlVBKAe0U8A5IV+AtSwUoeXiZJ2k9RX0gpJP29uQzMbY2ZzzSz86SEA9Yp+AiAvRfUTegkQr5KGF3df5e5funuDpMmS+ge2neTu/dy9X6lFAqhd9BMAeSm2n9BLgHiVNLyYWfdGPw6TtKC5bQEghH4CIC/0E6D2pa7zYma/kjRIUmczWyrpZ5IGmVlfSS5pkaRTW7HGmnPooYcG85tvvrnVa/jjH/8YzG+66aZgnnaN8GKuQ476Qz+pPpttFn4N68ILL0zdx/DhwzPVsHTp0mB+xx13ZNo/ahP9JF9Dhw4N5u4ezGfOnJlnORVx9913p26z9dZbB/P9998/mPfu3TuYT506NZi/+eabwbzYbWKWOry4+/FN3My/JABajH4CIC/0E6A+ZbnaGAAAAACUDcMLAAAAgCgwvAAAAACIAsMLAAAAgCgwvAAAAACIAsMLAAAAgCgwvAAAAACIgqUtOpTrwczKd7AK2XzzzVO3eeyxx4L5oEGDMtWQtgClJB122GHB/H//93+Deb9+/YL5Sy+9FMw7d+4czCXpo48+St0GuZjn7uH/oFWoHvpJOfz93/99MH/++eczH2PZsmXB/JBDDgnm7777buYaUB7ubpWuoaXqoZe0a9cudZvf/e53wfyzzz4L5mmLM5bDXnvtFcxnzJgRzIt5TvyP//iPwXzvvfcO5sccc0ymGkaNGhXMa0izz0048wIAAAAgCgwvAAAAAKLA8AIAAAAgCgwvAAAAAKLA8AIAAAAgCgwvAAAAAKLA8AIAAAAgCm0rXUCt6dmzZ+o2WddxWb16dTD/4Q9/mLqPtHVcWtuPfvSj1G1+8YtflKESoL5NnDix1Y+RtlbMCSec0Oo11IIPP/wwmN9zzz3B/OOPP86zHEQkbf0TSdpzzz2D+fTp0/Mqp2TDhg0L5nfddVcwf/zxx4P5Qw89lFrDmDFjgnlajc8991wwr6N1XErGmRcAAAAAUWB4AQAAABAFhhcAAAAAUWB4AQAAABAFhhcAAAAAUWB4AQAAABAFhhcAAAAAUWCdl5zlcX3uhoaGYH766acH83nz5mWuoX379sH8hhtuyHwMAJV38803B/PJkydnPsaIESMy7wPpjjrqqGB+6KGHlqkSVBszy7xN2npNeTjooIOC+W9+85tg7u7BvHfv3sE8bY0WSXr55ZeD+ciRI4N5Oe7HWseZFwAAAABRYHgBAAAAEAWGFwAAAABRYHgBAAAAEAWGFwAAAABRYHgBAAAAEAWGFwAAAABRYJ2XnG2zzTaZ95G2TsvMmTMzH2OfffYJ5mlrPxx44IGZjv/4449n+n0A+XjwwQeDef/+/VP38eMf/ziYF7PGRK0r5j5IW6MizQ9+8INMv4/aVcz/W2nb/PM//3Mwf/3114N52horxRwjrca0fM899wzmV199dTCX0p8frVmzJnUfyCb1zIuZ9TCzOWb2upktNLOzkts7mtlsM3sn+bND65cLIGb0EwB5oZ8A9amYt41tlHSuu/eW9F1J/2JmvSVdJOlJd99d0pPJzwAQQj8BkBf6CVCHUocXd1/h7i8n36+T9IaknSUdLWlastk0SUNbq0gAtYF+AiAv9BOgPrXoA/tm1kvSvpJektTN3Vck0UpJ3XKtDEBNo58AyAv9BKgfRX9g38zaS/qNpLPd/dPGHz50dzezJj8lZWZjJI3JWiiA2kE/AZCXUvoJvQSIV1FnXsxscxUaw73uPiO5eZWZdU/y7pJWN/W77j7J3fu5e788CgYQN/oJgLyU2k/oJUC8irnamEm6Q9Ib7n5Do2iWpNHJ96MlPZx/eQBqCf0EQF7oJ0B9srRrYpvZAEnPSXpNUkNy81gV3lf6gKRdJS2WNNzdP0zZV7aL2Edgv/32S93mpZdeCuazZ88O5k8++WQwP/HEE1Nr2G233YJ5+/btU/cRMn/+/GBezNoRX375ZaYaULR55Xr1kX5SmwYMGBDMhw0bFsy7du2aZzlN+uKLL4L53Xff3eo1pBk8eHAw33HHHTPtf+TIkZl+vxjuXrZFffLqJ/SSgksuuSSYX3XVVcG8oaEhmOex1tGSJUuC+bPPPhvMR40alVoDqkazz01SP/Pi7s9Lau7/uO9nqQpAfaGfAMgL/QSoTy262hgAAAAAVArDCwAAAIAoMLwAAAAAiALDCwAAAIAoMLwAAAAAiALDCwAAAIAoMLwAAAAAiELqIpW5HoyFoCSlL4Z2wgknlKmS5i1fvjyYjx07Npg/+uijwXzt2rUtrgmtpmyLVOaJfgJUn3IuUpkXeklxDjvssGA+dOjQzMdIW9B2w4YNwfycc84J5jNnzmxxTaiYZp+bcOYFAAAAQBQYXgAAAABEgeEFAAAAQBQYXgAAAABEgeEFAAAAQBQYXgAAAABEgeEFAAAAQBRY56UCjj322GB+//33Z9r/559/nrrNv/3bvwXzKVOmBPNly5a1qCZUNdZ5AZAL1nkBkBPWeQEAAAAQN4YXAAAAAFFgeAEAAAAQBYYXAAAAAFFgeAEAAAAQBYYXAAAAAFFgeAEAAAAQBdZ5AcA6LwBywTovAHLCOi8AAAAA4sbwAgAAACAKDC8AAAAAosDwAgAAACAKDC8AAAAAosDwAgAAACAKDC8AAAAAosDwAgAAACAKqcOLmfUwszlm9rqZLTSzs5Lbx5vZMjObn3wNbv1yAcSMfgIgD/QSoH61LWKbjZLOdfeXzWxbSfPMbHaSTXD361uvPAA1hn4CIA/0EqBOpQ4v7r5C0ork+3Vm9oaknVu7MAC1h34CIA/0EqB+tegzL2bWS9K+kl5KbjrDzF41sylm1iHn2gDUMPoJgDzQS4D6UvTwYmbtJf1G0tnu/qmkiZJ2k9RXhVc/ft7M740xs7lmNjeHegHUAPoJgDzQS4D6Y+6evpHZ5pIekfS4u9/QRN5L0iPuvk/KftIPBqDc5rl7v3IdjH4C1C53t3Idi14C1LRmn5sUc7Uxk3SHpDcaNwcz695os2GSFmStEkBto58AyAO9BKhfxVxt7EBJIyW9Zmbzk9vGSjrezPpKckmLJJ3aKhUCqCX0EwB5oJcAdaqot43ldjBOzQLVqKxvG8sL/QSoPuV821he6CVAVSr9bWMAAAAAUA0YXgAAAABEgeEFAAAAQBQYXgAAAABEgeEFAAAAQBQYXgAAAABEgeEFAAAAQBQYXgAAAABEgeEFAAAAQBQYXgAAAABEgeEFAAAAQBQYXgAAAABEgeEFAAAAQBQYXgAAAABEgeEFAAAAQBTalvl4ayQtbvRz5+S2akaN+aDGfLRGjT1z3l+50E9aBzXmox5rpJeUVwx1UmM+6rHGZvuJuXuOx2kZM5vr7v0qVkARqDEf1JiPGGqslBjuG2rMBzXmI4YaKyGW+yWGOqkxH9T4dbxtDAAAAEAUGF4AAAAARKHSw8ukCh+/GNSYD2rMRww1VkoM9w015oMa8xFDjZUQy/0SQ53UmA9qbKSin3kBAAAAgGJV+swLAAAAABSF4QUAAABAFCo2vJjZ4Wb2lpm9a2YXVaqOEDNbZGavmdl8M5tb6XokycymmNlqM1vQ6LaOZjbbzN5J/uxQhTWON7NlyX0538wGV7jGHmY2x8xeN7OFZnZWcnvV3JeBGqvqvqw0eknp6Ce51EcvqSH0k9LQS3KrkX5STA2V+MyLmbWR9Lakf5C0VNLvJR3v7q+XvZgAM1skqZ+7V83CQGZ2kKT1ku5y932S266T9KG7X5M02w7ufmGV1The0np3v75SdTVmZt0ldXf3l81sW0nzJA2VdJKq5L4M1DhcVXRfVhK9JBv6SXb0ktpBPykdvSQf9JPiVOrMS39J77r7++7+uaT7JB1doVqi4u7PSvpwk5uPljQt+X6aCv8TVUwzNVYVd1/h7i8n36+T9IaknVVF92WgRvwVvSQD+kl29JKaQj8pEb0kH/ST4lRqeNlZ0pJGPy9VdTZSl/SEmc0zszGVLiagm7uvSL5fKalbJYsJOMPMXk1O3Vb09HFjZtZL0r6SXlKV3peb1ChV6X1ZAfSS/FXlY6AJVfcYoJdEj36Sr6p8DDShKh8D9JPm8YH9sAHuvp+kIyT9S3LKsap54X2A1Xj964mSdpPUV9IKST+vbDkFZtZe0m8kne3unzbOquW+bKLGqrwvERRdL5Gq5zHQhKp7DNBLUEbR9ZNqeQw0oSofA/STsEoNL8sk9Wj08y7JbVXF3Zclf66WNFOFU8rVaFXyHsSv3ou4usL1/A13X+XuX7p7g6TJqoL70sw2V+GBd6+7z0hurqr7sqkaq/G+rCB6Sf6q6jHQlGp7DNBLagb9JF9V9RhoSjU+Bugn6So1vPxe0u5m9g0z20LScZJmVaiWJplZu+SDSDKzdpIOlbQg/FsVM0vS6OT70ZIermAtTfrqQZcYpgrfl2Zmku6Q9Ia739Aoqpr7srkaq+2+rDB6Sf6q5jHQnGp6DNBLagr9JF9V8xhoTrU9BugnRdbgFbjamCRZ4RJqN0pqI2mKu19dkUKaYWbfVOEVDUlqK2l6NdRoZr+SNEhSZ0mrJP1M0kOSHpC0q6TFkoa7e8U+lNZMjYNUOJXokhZJOrXR+zfLzswGSHpO0muSGpKbx6rwvs2quC8DNR6vKrovK41eUjr6SS710UtqCP2kNPSSfNBPiqyhUsMLAAAAALQEH9gHAAAAEAWGFwAAAABRYHgBAAAAEAWGFwAAAABRYHgBAAAAEAWGFwAAAABRYHgBAAAAEAWGFwAAAABRYHgBAAAAEAWGFwAAAABRYHgBAAAAEAWGFwAAAABRYHgBAAAAEAWGFwAAAABRYHgBAAAAEAWGFwAAAABRYHgBAAAAEAWGF/ct9tMAACAASURBVAAAAABRYHgBAAAAEAWGFwAAAABRYHgBAAAAEAWGFwAAAABRYHgBAAAAEAWGFwAAAABRYHgBAAAAEAWGFwAAAABRYHgBAAAAEAWGFwAAAABRYHgBAAAAEAWGFwAAAABRYHgBAAAAEAWGFwAAAABRYHgBAAAAEAWGFwAAAABRYHgBAAAAEAWGF3yNmY03s3ty3mcvM3Mza1vO3wVQWfQTAHmgl6AxhpcqY2ZPm9lHZrZlkdufZGbPt3ZdybEGmdnSchyrVElDeTS5D1ea2S00F9Qr+kk2ZnaPma0ws0/N7G0z+3GlawIqgV6SDc9N8sXwUkXMrJekgZJc0lEVLSZet0laLam7pL6SDpZ0ekUrAiqAfpKLf5fUy923U+E+vMrMvlPhmoCyopfkgucmOWJ4qS6jJL0oaaqk0Y0DM+thZjPM7I9mtjaZ2veWdLuk/2Nm683s42Tbpxu/QrjpKyBmdpOZLUleTZxnZgOzFm5mR5rZK8k+l5jZ+CY2O9nMlievZJ7X6Hc3M7OLzOy95O/2gJl1LLGUb0h6wN3/7O4rJT0m6dsl7guIGf0kYz9x94Xu/pevfky+ditlX0DE6CU8N6kqDC/VZZSke5Ovw8ysmySZWRtJj0haLKmXpJ0l3efub0g6TdIL7t7e3Xco8ji/V2Hy7yhpuqRfm9lWGWv/U1L/DpKOlPQTMxu6yTaHSNpd0qGSLjSzHyS3nylpqAqvROwk6SNJtzZ1kKSRPBKo40ZJx5nZNma2s6QjVGgSQL2hn2TvJzKz28zsM0lvSloh6dGS/kZAvOglPDepKgwvVcLMBkjqqcJkPk/Se5JOSOL+Kjxwznf3PyWTe8nvJXX3e9x9rbtvdPefS9pS0p5Z6nf3p939NXdvcPdXJf1KhQd8Y5cn9b8m6U5Jxye3nybpEndfmrzKOV7SsU29H9Tdr3H3HwZKeVaFVzM+lbRU0lxJD2X5uwGxoZ/k1k/k7qdL2laFt83MkPSX0PZALaGX8NykGjG8VI/Rkp5w9zXJz9P119OzPSQtdveNeRzIzM4zszfM7JPkdO72kjpn3OcBZjYnOXX8iQoP+k33uaTR94tVaHpSoTHONLOPk3rekPSlpG4trGEzFV7JmCGpXXL8DpKubenfB4gc/SRjP2nM3b9MnpTtIuknpe4HiBC9hOcmVYfhpQqY2daShks62ApXoVgp6V8l9TGzPio8sHZtatpX4T3Ym/qTpG0a/bxjo2MNlHRBcrwOyencTyRZxr/GdEmzJPVw9+1VeL/rpvvs0ej7XSUtT75fIukId9+h0ddW7r6shTV0TPZ7i7v/xd3XqvAqyuCW/mWAWNFPcusnTWkrPvOCOkEv4blJtWJ4qQ5DVZjme6vwfs++kvaW9JwK79X8nQrvtb7GzNqZ2VZmdmDyu6sk7WJmWzTa33xJxyTvrfyWpFMaZdtK2ijpj5LamtllkrZrSbHJ8Rt/WbLfD939z2bWX389rdzYpUlN35b0T5LuT26/XdLVZtYz2X8XMzu6JTVJUvLK0B9UeE9rWzPbQYVXiF5t6b6AiNFPcugnZtbVzI4zs/Zm1sbMDlPh7SRPtnRfQKToJTw3qUoML9VhtKQ73f0Dd1/51ZekWySdqMKrBEMkfUvSByq8X3JE8rtPSVooaaWZfXVad4Kkz1VoHtNU+JDdVx5X4fTl2yqcHv2zvn7KNM3OkjZs8rWbCpf8u8LM1km6TNIDTfzuM5LeVeEf/+vd/Ynk9ptUeGXkieT3X5R0QFMHN7OxZvbbQH3HSDpchQb4rqQvVHilCKgX9JN8+omr8BaxpSp8UPd6SWe7+6wW/P2AmNFLeG5Slcy9qTN7AAAAAFBdOPMCAAAAIAoMLwAAAACiwPACAAAAIAoMLwAAAACi0NS1uVuNmXF1AKD6rHH3LpUuoqXoJ0D1cfes63KUHb0EqErNPjfJdObFzA43s7fM7F0zuyjLvgBUzOJKFyDRTwDkh34CRK/Z5yYlDy9m1kbSrZKOUGEBo+PNrHep+wNQv+gnAPJCPwFqW5YzL/0lvevu77v755Luk9TilUcBQPQTAPmhnwA1LMvwsrO+vvrp0uS2rzGzMWY218zmZjgWgNpGPwGQl9R+Qi8B4tXqH9h390mSJkl8KA5ANvQTAHmglwDxynLmZZmkHo1+3iW5DQBain4CIC/0E6CGZRlefi9pdzP7hpltIek4SbPyKQtAnaGfAMgL/QSoYSW/bczdN5rZGZIel9RG0hR3X5hbZQDqBv0EQF7oJ0BtM/fyvdWT95UCVWmeu/erdBEtRT8Bqg+LVALISbPPTTItUgkAAAAA5cLwAgAAACAKDC8AAAAAosDwAgAAACAKDC8AAAAAosDwAgAAACAKDC8AAAAAosDwAgAAACAKDC8AAAAAosDwAgAAACAKDC8AAAAAosDwAgAAACAKDC8AAAAAosDwAgAAACAKDC8AAAAAosDwAgAAACAKDC8AAAAAosDwAgAAACAKDC8AAAAAosDwAgAAACAKDC8AAAAAosDwAgAAACAKDC8AAAAAotC20gUAAKrXtttum7rNrFmzgvmgQYOCeUNDQ0tKKsnDDz8czJ977rlgPnny5GC+fv36FtcEAGg5zrwAAAAAiALDCwAAAIAoMLwAAAAAiALDCwAAAIAoMLwAAAAAiALDCwAAAIAoMLwAAAAAiIK5e/kOZla+gyF6u+22WzDv1atXMH/ttdeC+erVq1ta0t/o1q1bMH/qqaeC+cSJE4P5Lbfc0uKaSjDP3fuV40B5op+UR48ePVK3+cMf/hDMzSyYl/Pfoeak1bh8+fJgftlllwXzO++8s8U1xcjdw3dkFaKXVI+ddtopmI8YMSKYH3XUUcE8hjWnhg0bFsyLqXHs2LHB/Nprr03dRxVo9rlJpkUqzWyRpHWSvpS0McYnQACqA/0EQF7oJ0DtyjS8JA5x9zU57AcA6CcA8kI/AWoQn3kBAAAAEIWsw4tLesLM5pnZmKY2MLMxZjbXzOZmPBaA2kY/AZCXYD+hlwDxyvq2sQHuvszMukqabWZvuvuzjTdw90mSJkl8KA5AEP0EQF6C/YReAsQr05kXd1+W/Lla0kxJ/fMoCkD9oZ8AyAv9BKhdJQ8vZtbOzLb96ntJh0pakFdhAOoH/QRAXugnQG0reZ0XM/umCq9mSIW3n01396tTfodTszUibY2VH/7wh8F81KhRqcfYZZddgvmOO+4YzB944IFgftxxxwXzffbZJ5hL0pw5c4L5q6++GswPO+ywYL5x48bUGnJQ8XVe6CfVq2vXrqnbzJs3L5inrd0QwzovaTWuW7cumJ9zzjmpNdTCWjDVsM5LS/sJvaQ8LrnkktRtzjjjjGDepUuXTDXUwppTxdT4wgsvBPOBAwe2qKYKyX+dF3d/X1KfkksCgAT9BEBe6CdAbeNSyQAAAACiwPACAAAAIAoMLwAAAACiwPACAAAAIAoMLwAAAACiwPACAAAAIAolr/NS0sG4lnpZpF0jXJK23nrrYH7WWWcF83HjxmXa/xdffBHMJel3v/tdML/uuuuC+XvvvRfM99hjj2B+9913B3NJ2nLLLYP5brvtFsyXLFmSeowyqPg6L6Wgn1SPPn3CV6U95ZRTgvmQIUOC+Q477BDM27RpE8wlaZtttgnmrb3+w4oVK1K36d27dzBPW0umGlTDOi8tRS/Jx0UXXRTML7vsstR9bLHFFplqWLRoUTBPexzedtttqcfYf//9g/lPf/rT1H2EpPWiNWvWpO7jpJNOCua//e1vW1JSpTT73IQzLwAAAACiwPACAAAAIAoMLwAAAACiwPACAAAAIAoMLwAAAACiwPACAAAAIAoMLwAAAACiwPACAAAAIAosUhmhb3/728F82LBhqfu44oorMtWwYcOGYD5p0qRgftNNN6UeI22xqTSnnXZaME9bjKqYBeEOO+ywYP7iiy+m7qMKsEglqtrBBx8czNu3b5+6j3POOSeYDxo0KJiX49/K8847L5jfeOONrV5DVixSWbvSFotdu3ZtMC/mMfT2228H87SFLufPnx/MV61aFczvu+++YC6l/7uf1fTp04N5MQtpRvLcIw2LVAIAAACIG8MLAAAAgCgwvAAAAACIAsMLAAAAgCgwvAAAAACIAsMLAAAAgCgwvAAAAACIQttKF1CPOnfuHMwvuOCCYH7GGWcE86222iq1ho8//jiYp13r/JprrgnmH3zwQWoNWR199NHBPG1NhGXLlgXzk08+ObWGGrmWOlDV3nnnnWA+YsSI1H1stln4tbq0vKGhIfUYQC0bN25cqx8jba2YBx98MNP+58yZE8wHDhyYaf/FePjhh4P5qFGjWr2G2HHmBQAAAEAUGF4AAAAARIHhBQAAAEAUGF4AAAAARIHhBQAAAEAUGF4AAAAARIHhBQAAAEAUWOclZwceeGDqNjfffHMw33fffYP5xo0bg3naOjGS9Otf/zqYL168OHUfWbRv3z51m9NPPz2YX3rppcF86dKlwfy8884L5rNnzw7mAIqzxx57BPO0tRu6dOmSKS9G2jou7p5p/5999lnqNm+99VamYwCtycyCeR5rJZ1//vnBfNtttw3ms2bNCuaDBg0K5nms53TvvfcGc9ZxyS71zIuZTTGz1Wa2oNFtHc1stpm9k/zZoXXLBFAL6CcA8kI/AepTMW8bmyrp8E1uu0jSk+6+u6Qnk58BIM1U0U8A5GOq6CdA3UkdXtz9WUkfbnLz0ZKmJd9PkzQ057oA1CD6CYC80E+A+lTqZ166ufuK5PuVkro1t6GZjZE0psTjAKh99BMAeSmqn9BLgHhl/sC+u7uZNftJRnefJGmSJIW2AwD6CYC8hPoJvQSIV6mXSl5lZt0lKflzdX4lAagz9BMAeaGfADWu1OFllqTRyfejJT2cTzkA6hD9BEBe6CdAjUt925iZ/UrSIEmdzWyppJ9JukbSA2Z2iqTFkoa3ZpExOfzwTS988rfS1nFJ8/DD4V58/fXXZ9p/Hnr16hXMp06dmrqPgw46KJinrYkwcuTIYD537tzUGpAv+kn12WeffYJ52npLktSnT59g3r1792C+6667ph6j2i1YsCCYn3TSSan7mD9/fk7V1Af6SXmlrXWUx1pJhxxySDBPW99t4MCBwTyPGqdMmRLMzzrrrNR9IJvU4cXdj28m+n7OtQCocfQTAHmhnwD1qdS3jQEAAABAWTG8AAAAAIgCwwsAAACAKDC8AAAAAIgCwwsAAACAKDC8AAAAAIgCwwsAAACAKKSu84KWufHGG1O3ad++fTBPW+Bo2LBhwfziiy9OreHf//3fU7cJ2WOPPYL5E088EcyLWZQubdG3tAVBly9fnnoMoNYdccQRwXzatGnBvGPHjplrMLNgXszCcJX2wgsvBPPLLrssmLMAJWL3wQcftPoxrrzyylY/RsjZZ5+duk3aItsbNmzIqRo0hzMvAAAAAKLA8AIAAAAgCgwvAAAAAKLA8AIAAAAgCgwvAAAAAKLA8AIAAAAgCgwvAAAAAKLAOi85W7t2beo25557bjD/5JNPgvm4ceOC+RVXXJFaw6BBg4L5JZdcEszvv//+YJ62jsvChQuDuSQNHjw4mLOOC5Du9ttvD+Z5rONSDzp16hTM33///TJVAlTGzTffHMy/9a1vBfNTTz019Rht2rRpUU15K+Y53Pr168tQCUI48wIAAAAgCgwvAAAAAKLA8AIAAAAgCgwvAAAAAKLA8AIAAAAgCgwvAAAAAKLA8AIAAAAgCubu5TuYWfkOVsPS1nFJWwemHB5//PFgfvLJJ6fuY8WKFXmVg7B57t6v0kW0FP2kOJdddlkwT1t7Yccdd8xcw2abhV8na2hoyHyMrLLW+D//8z/BfODAgS2uKUbubpWuoaXoJeWxePHi1G123nnnVq3BLPy/51tvvZW6j7S1s9LWw0HRmn1uwpkXAAAAAFFgeAEAAAAQBYYXAAAAAFFgeAEAAAAQBYYXAAAAAFFgeAEAAAAQBYYXAAAAAFFgnZcadN1116Vuc9555wXztGuhr1+/PpgfcMABwfz1118P5igr1nmpY126dAnmXbt2Td1H2tpSaf0mTa9evYL5BRdckLqPI488Mpin9by0fytfeOGFYM46L9WLXpKPhx56KJgPGTKkTJU0rxxrTo0YMSKYP/jgg5mPUSdKX+fFzKaY2WozW9DotvFmtszM5idfg/OsFkBtop8AyAv9BKhPxbxtbKqkw5u4fYK7902+Hs23LAA1aqroJwDyMVX0E6DupA4v7v6spA/LUAuAGkc/AZAX+glQn7J8YP8MM3s1OW3bobmNzGyMmc01s7kZjgWgttFPAOQltZ/QS4B4lTq8TJS0m6S+klZI+nlzG7r7JHfvF+MHggGUBf0EQF6K6if0EiBeJQ0v7r7K3b909wZJkyX1z7csAPWCfgIgL/QToPaVNLyYWfdGPw6TtKC5bQEghH4CIC/0E6D2pa7zYma/kjRIUmdJqyT9LPm5rySXtEjSqe6+IvVgXEs9F9/97neD+TPPPJO6j8033zyvcpp0zDHHBPO068GjrMq2zgv9BK1hyy23TN1mypQpwfy4444L5qzzUpxyrvOSVz+hlxRss802wfyXv/xlMD/hhBOCeTHrCqY9Ti+//PJgftRRRwXzW265JZjnsfbhwoULg3mfPn0yH6NONPvcpG3ab7r78U3cfEfmkgDUHfoJgLzQT4D6lOVqYwAAAABQNgwvAAAAAKLA8AIAAAAgCgwvAAAAAKLA8AIAAAAgCgwvAAAAAKLA8AIAAAAgCqnrvKD8evbsGcynT58ezItZgPKtt94K5hMmTAjmt912WzBPW8yKRSoB5OXLL79M3Wb9+vWtWsPHH3/cqvsHWtv48eOD+fHHN7WsTvGeeuqp1G3OOeecYJ72OJ44cWIwf+WVV4L5zJkzg7kkdenSJVPevXv3YL5iReoazXWPMy8AAAAAosDwAgAAACAKDC8AAAAAosDwAgAAACAKDC8AAAAAosDwAgAAACAKDC8AAAAAosA6LxXQpk2bYH7VVVcF8169egXz22+/PbWGK6+8MpinXWf83HPPDea77757MB80aFAwl6Snn346dRsAtS+t51188cWp+zjllFNyqqZpaX0bqKT+/funbjN8+PBWreG9995L3aa112N68cUXg/nnn3+e+RidOnUK5oMHDw7md9xxR+Yaah1nXgAAAABEgeEFAAAAQBQYXgAAAABEgeEFAAAAQBQYXgAAAABEgeEFAAAAQBQYXgAAAABEgXVeKmDAgAHB/MQTTwzmCxYsCOY//elPU2vYuHFj6jatqWfPnhU9PlAOO+20UzAfMWJEq9cwefLkYD5kyJBgvuOOO+ZZTpMuvfTSYL799tu3eg2bbRZ+LW+XXXYJ5suWLcuzHCBXEyZMSN0m7f/xNM8880wwP//88zPtPw8HH3xwMM+j1yxfvjyYs45Ldpx5AQAAABAFhhcAAAAAUWB4AQAAABAFhhcAAAAAUWB4AQAAABAFhhcAAAAAUWB4AQAAABAF1nmpgAsuuCDT799///3BvNJruBTjP//zPytdApDZSy+9FMx33XXXYN6lS5fMNZhZMD/nnHOCeadOnYL5Flts0eKaNpVWo7tnyvPw5ptvBvMNGza0eg1AqYYPHx7Mv/vd77Z6Dd/73vda/RhZPf3008G8oaEh8zGqYT2bWpd65sXMepjZHDN73cwWmtlZye0dzWy2mb2T/Nmh9csFEDP6CYC80E+A+lTM28Y2SjrX3XtL+q6kfzGz3pIukvSku+8u6cnkZwAIoZ8AyAv9BKhDqcOLu69w95eT79dJekPSzpKOljQt2WyapKGtVSSA2kA/AZAX+glQn1r0mRcz6yVpX0kvSerm7iuSaKWkbs38zhhJY0ovEUAtop8AyEtL+wm9BIhX0VcbM7P2kn4j6Wx3/7Rx5oVPUzb5iUp3n+Tu/dy9X6ZKAdQM+gmAvJTST+glQLyKGl7MbHMVGsO97j4juXmVmXVP8u6SVrdOiQBqCf0EQF7oJ0D9KeZqYybpDklvuPsNjaJZkkYn34+W9HD+5QGoJfQTAHmhnwD1qZjPvBwoaaSk18xsfnLbWEnXSHrAzE6RtFhS+CLj+P86dMh21caJEyfmVEnz9tprr2C+0047Zdr/hx9+mOn3Ea2o+kna/+flWMclq+7du1e6hFb3l7/8JXWbyy+/PJinrZ9Fz6pKUfWT1lSOtZKuvPLKzPvIKm29mh49egTztHVcyrGmFLJLHV7c/XlJza0w9v18ywFQy+gnAPJCPwHqU9Ef2AcAAACASmJ4AQAAABAFhhcAAAAAUWB4AQAAABAFhhcAAAAAUWB4AQAAABAFhhcAAAAAUShmkUpUmaOOOiqYT506NXUfnTp1CubTp08P5u3atQvmEyZMSK0BqHYjRowI5tWwCGUtWL58eTB///33g/m1116beozf/va3LaoJwNeNGjUqmG+11VbBfPHixanHGDlyZDDffffdg3nHjh1TjxGydu3a1G3OPPPMYD5nzpxMNSAdZ14AAAAARIHhBQAAAEAUGF4AAAAARIHhBQAAAEAUGF4AAAAARIHhBQAAAEAUGF4AAAAARIF1Xirg7rvvDuZ/93d/F8wnTpwYzC+++OLUGtq3bx/Mu3fvHsyfeOKJYH7rrbem1gBUu2eeeSaY33XXXcE8bV2EarBo0aJgfsstt7R6Df/xH/8RzNevX9/qNQAx27BhQzBfvXp16j66du0azHv27BnML7jggmDu7qk1tLalS5cG8zFjxqTu4/HHH8+rHJSIMy8AAAAAosDwAgAAACAKDC8AAAAAosDwAgAAACAKDC8AAAAAosDwAgAAACAKDC8AAAAAomDlvO62mVX+It8ROPzww4P5T37yk2A+ZMiQzDXcc889wTzteu4rV67MXAPKZp6796t0ES1FPwGqj7tbpWtoqXroJWlrt0nSI488Esz79OkTzM3C/+nzeL758ccfB/OrrroqmN94442Za0DZNPvchDMvAAAAAKLA8AIAAAAgCgwvAAAAAKLA8AIAAAAgCgwvAAAAAKLA8AIAAAAgCgwvAAAAAKLAOi8AWOcFQC5Y5wVATkpf58XMepjZHDN73cwWmtlZye3jzWyZmc1PvgbnXTWA2kI/AZAHeglQv9oWsc1GSee6+8tmtq2keWY2O8kmuPv1rVcegBpDPwGQB3oJUKdShxd3XyFpRfL9OjN7Q9LOrV0YgNpDPwGQB3oJUL9a9IF9M+slaV9JLyU3nWFmr5rZFDPr0MzvjDGzuWY2N1OlAGoK/QRAHuglQH0p+gP7ZtZe0jOSrnb3GWbWTdIaSS7pSknd3f3klH3woTig+pT9A/v0E6A2lfsD+/QSoGaV/oF9STKzzSX9RtK97j5Dktx9lbt/6e4NkiZL6p9XtQBqF/0EQB7oJUB9KuZqYybpDklvuPsNjW7v3mizYZIW5F8egFpCPwGQB3oJUL+KudrYgZJGSnrNzOYnt42VdLyZ9VXh1OwiSae2SoUAagn9BEAe6CVAnWKRSgAsUgkgFyxSCSAn2T7zAgAAAACVxvACAAAAIAoMLwAAAACiwPACAAAAIAoMLwAAAACiwPACAAAAIAoMLwAAAACiwPACAAAAIAoMLwAAAACiwPACAAAAIAoMLwAAAACiwPACAAAAIAoMLwAAAACiwPACAAAAIAoMLwAAAACi0LbMx1sjaXGjnzsnt1UzaswHNeajNWrsmfP+yoV+0jqoMR/1WCO9pLxiqJMa81GPNTbbT8zdczxOy5jZXHfvV7ECikCN+aDGfMRQY6XEcN9QYz6oMR8x1FgJsdwvMdRJjfmgxq/jbWMAAAAAosDwAgAAACAKlR5eJlX4+MWgxnxQYz5iqLFSYrhvqDEf1JiPGGqshFjulxjqpMZ8UGMjFf3MCwAAAAAUq9JnXgAAAACgKBUbXszscDN7y8zeNbOLKlVHiJktMrPXzGy+mc2tdD2SZGZTzGy1mS1odFtHM5ttZu8kf3aowhrHm9my5L6cb2aDK1xjDzObY2avm9lCMzsrub1q7stAjVV1X1YavaR09JNc6qOX1BD6SWnoJbnVSD8ppoZKvG3MzNpIelvSP0haKun3ko5399fLXkyAmS2S1M/dq+ba2mZ2kKT1ku5y932S266T9KG7X5M02w7ufmGV1The0np3v75SdTVmZt0ldXf/f+3dMYwVVRSH8e9EoUE7EgrAgIQeKawoqEio0IZgBZUWUFDTUFlK6CyMJBQqIVGU0lIqYqBAEhpjNEKWpbCQzkSOxQzxBecN6+6Ee+bl+zU7b142OTnZ80/u3jvv5d2IeBO4A7wHnKFIL0dqPEmhXrZklmyNebJ1ZsnqME82zyyZhnmyMa12Xt4Ffs7MXzLzL+AacKJRLbOSmT8Af7xw+wRwtb++SvdH1MySGkvJzLXMvNtfPwUeALsp1MuRGvUvs2QLzJOtM0tWinmySWbJNMyTjWm1eNkN/L7w+iE1gzSB7yPiTkR82LqYEbsyc62/fgzsalnMiHMRca/fum26fbwoIvYB7wC3KdrLF2qEor1swCyZXskZGFBuBsyS2TNPplVyBgaUnAHzZDkf2B93JDMPA8eBs/2WY2nZnQOs+BFynwIHgEPAGvBJ23I6EfEG8DVwPjP/XHyvSi8HaizZS42aXZZAnRkYUG4GzBK9QrPLkyozMKDkDJgn41otXh4Bexde7+nvlZKZj/qfT4AbdFvKFa33ZxCfn0V80rie/8jM9cz8OzOfAZ9RoJcRsY1u8L7IzG/626V6OVRjxV42ZJZMr9QMDKk2A2bJyjBPplVqBoZUnAHz5OVaLV5+BA5GxP6I2A6cAm42qmVQROzoH0QiInYAx4D747/VzE3gdH99GviuYS2Dng9d730a9zIiAvgceJCZlxbeKtPLZTVW62VjZsn0yszAqtn7gQAAANlJREFUMpVmwCxZKebJtMrMwDLVZsA82WAN2ehLKqP7CLXLwGvAlcz8uEkhS0TE23T/0QB4HfiyQo0R8RVwFNgJrAMXgW+B68BbwG/Aycxs9lDakhqP0m0lJvAr8NHC+c1XLiKOALeAn4Bn/e0LdOc2S/RypMYPKNTL1sySzTNPJqnPLFkh5snmmCXTME82WEOrxYskSZIk/R8+sC9JkiRpFly8SJIkSZoFFy+SJEmSZsHFiyRJkqRZcPEiSZIkaRZcvEiSJEmaBRcvkiRJkmbBxYskSZKkWfgHQoWD9M7dYSwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1008x720 with 6 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r50FevcIox4-",
        "colab_type": "text"
      },
      "source": [
        "Next we will define the structure of our convolutional network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOGUF3tJoEu4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aodhaumzpKn8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Network(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Network, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 32, kernel_size=  3, stride = 1)\n",
        "    self.conv2 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 3, stride = 1)\n",
        "    self.pool = nn.MaxPool2d(2, stride = 2)\n",
        "    self.pool_dropout = nn.Dropout2d(p = 0.25)\n",
        "    self.fc1 = nn.Linear(in_features = 9216, out_features = 128)\n",
        "    self.fc_dropout = nn.Dropout(p = 0.5) \n",
        "    self.fc2 = nn.Linear(in_features = 128, out_features = 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = self.pool_dropout(self.pool(x))\n",
        "    x = x.view(-1, 9216)\n",
        "    x = F.relu(self.fc_dropout(self.fc1(x)))\n",
        "    x = F.log_softmax(self.fc2(x), dim = 1)\n",
        "    return x\n",
        "\n",
        "net = Network()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBqZ33UjuAcN",
        "colab_type": "text"
      },
      "source": [
        "Next, lets define the loss and optimizer for the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8hBgL-Dt6Nm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.003, momentum = 0.7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qxv0i2vwZaLF",
        "colab_type": "text"
      },
      "source": [
        "Define the train function to train the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVpL5_oE8Z_4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epoch):\n",
        "  net.train()\n",
        "  for batch_idx, (data, target) in enumerate(trainloader):\n",
        "    optimizer.zero_grad()\n",
        "    output = net(data)\n",
        "    loss = criterion(output, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if batch_idx % 20 == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "        epoch, batch_idx * len(data), len(trainloader.dataset),\n",
        "        100. * batch_idx / len(trainloader), loss.item()))\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MY7bqJHsZqNJ",
        "colab_type": "text"
      },
      "source": [
        "Define the function to calculate the training loss for the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lb8reQjMS6fn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_acc():\n",
        "  net.eval()\n",
        "  train_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data, target in trainloader:\n",
        "      output = net(data)\n",
        "      train_loss += criterion(output, target).item()\n",
        "      pred = output.data.max(1, keepdim=True)[1]\n",
        "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "  train_loss /= len(testloader.dataset)\n",
        "  print('\\nTrain set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "    train_loss, correct, len(trainloader.dataset),\n",
        "    100. * correct / len(trainloader.dataset)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJEnN59NZwcR",
        "colab_type": "text"
      },
      "source": [
        "Define the function to calculate the test loss for the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQ3NNy9u-6ZT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_acc():\n",
        "  net.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data, target in testloader:\n",
        "      output = net(data)\n",
        "      test_loss += criterion(output, target).item()\n",
        "      pred = output.data.max(1, keepdim=True)[1]\n",
        "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "  test_loss /= len(testloader.dataset)\n",
        "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "    test_loss, correct, len(testloader.dataset),\n",
        "    100. * correct / len(testloader.dataset)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VgyI0vFZy2Z",
        "colab_type": "text"
      },
      "source": [
        "And finally, train the model for 10 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wj3QUk5G_QxF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "72a321f1-6a5d-4450-e79c-1b414ab94191"
      },
      "source": [
        "test_acc()\n",
        "for epoch in range(1, 10 + 1):\n",
        "  train(epoch)\n",
        "  train_acc()\n",
        "  test_acc()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Avg. loss: 0.0362, Accuracy: 1123/10000 (11%)\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.339756\n",
            "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.155152\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 1.854377\n",
            "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 1.392816\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.923087\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.092260\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.758474\n",
            "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.856583\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.745803\n",
            "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.573274\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.431828\n",
            "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.671961\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.576946\n",
            "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.423588\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.309418\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.349216\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.406852\n",
            "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.372329\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.307011\n",
            "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.623810\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.270029\n",
            "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.373258\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.259338\n",
            "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.299143\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.417009\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.363555\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.374967\n",
            "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.465730\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.530047\n",
            "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.330756\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.455320\n",
            "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.542774\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.308659\n",
            "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.568400\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.397074\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.477795\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.371782\n",
            "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.362378\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.390678\n",
            "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.393961\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.443740\n",
            "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.200885\n",
            "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.347497\n",
            "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.391978\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.257748\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.270945\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.138887\n",
            "\n",
            "Train set: Avg. loss: 0.0198, Accuracy: 56190/60000 (94%)\n",
            "\n",
            "\n",
            "Test set: Avg. loss: 0.0031, Accuracy: 9381/10000 (94%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.278053\n",
            "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.281680\n",
            "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.166141\n",
            "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.384414\n",
            "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.163931\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.350166\n",
            "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.382715\n",
            "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.203745\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.536163\n",
            "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.238499\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.249946\n",
            "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.215197\n",
            "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.472949\n",
            "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.296389\n",
            "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.208993\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.229294\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.381899\n",
            "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.414446\n",
            "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.228537\n",
            "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.204573\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.296868\n",
            "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.492471\n",
            "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.276346\n",
            "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.274243\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.187489\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.381336\n",
            "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.330711\n",
            "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.275984\n",
            "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.238724\n",
            "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.367085\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.210560\n",
            "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.377826\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.230038\n",
            "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.295755\n",
            "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.126658\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.486899\n",
            "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.277327\n",
            "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.534977\n",
            "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.182538\n",
            "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.282029\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.133889\n",
            "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.230256\n",
            "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.187373\n",
            "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.258903\n",
            "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.290147\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.203344\n",
            "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.093443\n",
            "\n",
            "Train set: Avg. loss: 0.0150, Accuracy: 57021/60000 (95%)\n",
            "\n",
            "\n",
            "Test set: Avg. loss: 0.0024, Accuracy: 9530/10000 (95%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.130191\n",
            "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.175888\n",
            "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.241407\n",
            "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.191231\n",
            "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.250710\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.174566\n",
            "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.390952\n",
            "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.234756\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.155335\n",
            "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.225247\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.107445\n",
            "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.289607\n",
            "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.158754\n",
            "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.134557\n",
            "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.226097\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.082406\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.271667\n",
            "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.195408\n",
            "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.196660\n",
            "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.321174\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.274845\n",
            "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.207021\n",
            "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.148821\n",
            "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.122477\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.182051\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.230900\n",
            "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.254132\n",
            "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.078441\n",
            "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.232206\n",
            "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.117935\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.314567\n",
            "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.212634\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.338804\n",
            "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.281101\n",
            "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.146388\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.244161\n",
            "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.162862\n",
            "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.203540\n",
            "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.312508\n",
            "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.130834\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.498314\n",
            "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.194320\n",
            "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.239199\n",
            "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.326254\n",
            "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.176291\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.246368\n",
            "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.142544\n",
            "\n",
            "Train set: Avg. loss: 0.0116, Accuracy: 57739/60000 (96%)\n",
            "\n",
            "\n",
            "Test set: Avg. loss: 0.0019, Accuracy: 9626/10000 (96%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.141169\n",
            "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.286512\n",
            "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.090197\n",
            "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.152563\n",
            "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.134930\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.130760\n",
            "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.203221\n",
            "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.158887\n",
            "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.189313\n",
            "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.121181\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.245770\n",
            "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.244386\n",
            "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.174487\n",
            "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.444985\n",
            "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.164303\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.212582\n",
            "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.133946\n",
            "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.249419\n",
            "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.209512\n",
            "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.072595\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.133657\n",
            "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.243682\n",
            "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.263459\n",
            "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.074472\n",
            "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.117824\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.198958\n",
            "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.129481\n",
            "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.130194\n",
            "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.217950\n",
            "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.203524\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.076236\n",
            "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.068507\n",
            "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.242657\n",
            "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.296839\n",
            "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.258195\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.121577\n",
            "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.056858\n",
            "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.307739\n",
            "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.261225\n",
            "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.289988\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.239711\n",
            "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.270618\n",
            "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.388734\n",
            "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.124656\n",
            "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.244896\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.342869\n",
            "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.160276\n",
            "\n",
            "Train set: Avg. loss: 0.0100, Accuracy: 58079/60000 (97%)\n",
            "\n",
            "\n",
            "Test set: Avg. loss: 0.0016, Accuracy: 9686/10000 (97%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.211336\n",
            "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.288479\n",
            "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.053569\n",
            "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.316136\n",
            "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.122224\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.188543\n",
            "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.141770\n",
            "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.208637\n",
            "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.157961\n",
            "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.108863\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.073778\n",
            "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.086188\n",
            "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.143313\n",
            "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.141239\n",
            "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.220879\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.052138\n",
            "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.186516\n",
            "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.101868\n",
            "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.209463\n",
            "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.126679\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.118794\n",
            "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.307323\n",
            "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.063346\n",
            "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.176006\n",
            "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.172734\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.143614\n",
            "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.067070\n",
            "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.115915\n",
            "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.187817\n",
            "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.095605\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.116683\n",
            "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.217721\n",
            "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.124600\n",
            "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.239201\n",
            "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.296272\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.291307\n",
            "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.134911\n",
            "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.153157\n",
            "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.141897\n",
            "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.176009\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.194187\n",
            "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.161052\n",
            "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.108371\n",
            "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.056145\n",
            "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.160551\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.265928\n",
            "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.173391\n",
            "\n",
            "Train set: Avg. loss: 0.0080, Accuracy: 58464/60000 (97%)\n",
            "\n",
            "\n",
            "Test set: Avg. loss: 0.0014, Accuracy: 9732/10000 (97%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.103649\n",
            "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.181009\n",
            "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.176486\n",
            "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.194481\n",
            "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.129784\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.142406\n",
            "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.055992\n",
            "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.168827\n",
            "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.086031\n",
            "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.294398\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.231623\n",
            "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.190804\n",
            "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.201204\n",
            "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.160091\n",
            "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.094782\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.228888\n",
            "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.190687\n",
            "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.110712\n",
            "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.192584\n",
            "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.195718\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.189185\n",
            "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.128948\n",
            "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.116304\n",
            "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.165201\n",
            "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.250813\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.111726\n",
            "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.116019\n",
            "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.261786\n",
            "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.177824\n",
            "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.115040\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.072590\n",
            "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.203065\n",
            "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.226756\n",
            "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.125208\n",
            "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.114436\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.111795\n",
            "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.181554\n",
            "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.061043\n",
            "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.178415\n",
            "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.058547\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.193972\n",
            "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.094798\n",
            "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.058293\n",
            "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.304819\n",
            "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.096305\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.154343\n",
            "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.162082\n",
            "\n",
            "Train set: Avg. loss: 0.0067, Accuracy: 58718/60000 (98%)\n",
            "\n",
            "\n",
            "Test set: Avg. loss: 0.0011, Accuracy: 9776/10000 (98%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.094429\n",
            "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.209632\n",
            "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.143658\n",
            "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.180613\n",
            "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.157533\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.090570\n",
            "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.127574\n",
            "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.083547\n",
            "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.073697\n",
            "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.095807\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.208374\n",
            "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.114072\n",
            "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.095776\n",
            "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.109152\n",
            "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.106658\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.040871\n",
            "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.159131\n",
            "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.146114\n",
            "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.166843\n",
            "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.046140\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.054044\n",
            "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.236012\n",
            "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.217721\n",
            "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.049688\n",
            "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.275336\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.139475\n",
            "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.066119\n",
            "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.067577\n",
            "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.209206\n",
            "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.166536\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.051462\n",
            "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.118924\n",
            "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.100670\n",
            "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.183549\n",
            "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.160535\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.093479\n",
            "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.173545\n",
            "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.141507\n",
            "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.056939\n",
            "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.180068\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.152105\n",
            "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.064046\n",
            "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.235043\n",
            "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.150728\n",
            "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.069278\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.188017\n",
            "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.026072\n",
            "\n",
            "Train set: Avg. loss: 0.0058, Accuracy: 58915/60000 (98%)\n",
            "\n",
            "\n",
            "Test set: Avg. loss: 0.0010, Accuracy: 9807/10000 (98%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.220725\n",
            "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.134295\n",
            "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.168710\n",
            "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.149592\n",
            "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.119617\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.113152\n",
            "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.052456\n",
            "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.198587\n",
            "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.224247\n",
            "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.193387\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.080691\n",
            "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.068982\n",
            "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.125766\n",
            "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.213584\n",
            "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.101444\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.046240\n",
            "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.184797\n",
            "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.069655\n",
            "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.062199\n",
            "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.081730\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.081443\n",
            "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.081329\n",
            "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.136318\n",
            "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.128066\n",
            "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.091298\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.202086\n",
            "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.134399\n",
            "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.136671\n",
            "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.111635\n",
            "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.088236\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.035615\n",
            "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.090903\n",
            "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.110867\n",
            "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.110250\n",
            "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.055415\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.147897\n",
            "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.101897\n",
            "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.089958\n",
            "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.055840\n",
            "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.017800\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.083467\n",
            "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.077688\n",
            "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.073904\n",
            "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.234973\n",
            "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.107870\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.047277\n",
            "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.104489\n",
            "\n",
            "Train set: Avg. loss: 0.0050, Accuracy: 59039/60000 (98%)\n",
            "\n",
            "\n",
            "Test set: Avg. loss: 0.0009, Accuracy: 9829/10000 (98%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.143310\n",
            "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.053524\n",
            "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.122715\n",
            "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.131095\n",
            "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.089252\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.065883\n",
            "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.097598\n",
            "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.034660\n",
            "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.073191\n",
            "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.075903\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.069457\n",
            "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.038227\n",
            "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.061935\n",
            "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.194954\n",
            "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.077229\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.046172\n",
            "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.037596\n",
            "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.053080\n",
            "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.167259\n",
            "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.081720\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.243280\n",
            "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.051043\n",
            "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.051916\n",
            "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.119696\n",
            "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.092941\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.258075\n",
            "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.073676\n",
            "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.078550\n",
            "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.192084\n",
            "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.086131\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.100983\n",
            "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.190577\n",
            "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.062735\n",
            "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.136404\n",
            "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.065860\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.101921\n",
            "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.164111\n",
            "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.052009\n",
            "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.065583\n",
            "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.104573\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.110994\n",
            "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.032336\n",
            "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.058639\n",
            "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.037567\n",
            "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.230198\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.054290\n",
            "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.039720\n",
            "\n",
            "Train set: Avg. loss: 0.0044, Accuracy: 59159/60000 (99%)\n",
            "\n",
            "\n",
            "Test set: Avg. loss: 0.0008, Accuracy: 9843/10000 (98%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.037985\n",
            "Train Epoch: 10 [1280/60000 (2%)]\tLoss: 0.047601\n",
            "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 0.036419\n",
            "Train Epoch: 10 [3840/60000 (6%)]\tLoss: 0.178856\n",
            "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 0.297593\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.045569\n",
            "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 0.053370\n",
            "Train Epoch: 10 [8960/60000 (15%)]\tLoss: 0.080586\n",
            "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 0.073907\n",
            "Train Epoch: 10 [11520/60000 (19%)]\tLoss: 0.039645\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.066739\n",
            "Train Epoch: 10 [14080/60000 (23%)]\tLoss: 0.193555\n",
            "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 0.113347\n",
            "Train Epoch: 10 [16640/60000 (28%)]\tLoss: 0.199953\n",
            "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 0.070724\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.093499\n",
            "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 0.021259\n",
            "Train Epoch: 10 [21760/60000 (36%)]\tLoss: 0.030067\n",
            "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 0.040708\n",
            "Train Epoch: 10 [24320/60000 (41%)]\tLoss: 0.029233\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.032110\n",
            "Train Epoch: 10 [26880/60000 (45%)]\tLoss: 0.020681\n",
            "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 0.062798\n",
            "Train Epoch: 10 [29440/60000 (49%)]\tLoss: 0.034735\n",
            "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.186060\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.111416\n",
            "Train Epoch: 10 [33280/60000 (55%)]\tLoss: 0.034518\n",
            "Train Epoch: 10 [34560/60000 (58%)]\tLoss: 0.100731\n",
            "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 0.069112\n",
            "Train Epoch: 10 [37120/60000 (62%)]\tLoss: 0.027299\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.106512\n",
            "Train Epoch: 10 [39680/60000 (66%)]\tLoss: 0.134797\n",
            "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 0.089776\n",
            "Train Epoch: 10 [42240/60000 (70%)]\tLoss: 0.050935\n",
            "Train Epoch: 10 [43520/60000 (72%)]\tLoss: 0.097101\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.106011\n",
            "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 0.055071\n",
            "Train Epoch: 10 [47360/60000 (79%)]\tLoss: 0.109665\n",
            "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 0.092263\n",
            "Train Epoch: 10 [49920/60000 (83%)]\tLoss: 0.141971\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.077190\n",
            "Train Epoch: 10 [52480/60000 (87%)]\tLoss: 0.109636\n",
            "Train Epoch: 10 [53760/60000 (90%)]\tLoss: 0.050596\n",
            "Train Epoch: 10 [55040/60000 (92%)]\tLoss: 0.062428\n",
            "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 0.043429\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.064716\n",
            "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 0.104561\n",
            "\n",
            "Train set: Avg. loss: 0.0039, Accuracy: 59296/60000 (99%)\n",
            "\n",
            "\n",
            "Test set: Avg. loss: 0.0007, Accuracy: 9857/10000 (99%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDmC0EoXaXVs",
        "colab_type": "text"
      },
      "source": [
        "So at the end of 10 epochs, we have train accuracy equal to and test accuracy equal to"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S71iu-LtaheK",
        "colab_type": "text"
      },
      "source": [
        "Lets try to see how the model performs on the individual labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5FKvzZ59Ain",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "af3e8554-f11f-46ce-94a7-9fa344051ac5"
      },
      "source": [
        "classes = (0, 1,2 ,3 ,4 ,5 ,6, 7, 8, 9)\n",
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        c = (predicted == labels).squeeze()\n",
        "        for i in range(4):\n",
        "            label = labels[i]\n",
        "            class_correct[label] += c[i].item()\n",
        "            class_total[label] += 1\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "    print('Accuracy of %5s : %2d %%' % (\n",
        "        classes[i], 100 * class_correct[i] / class_total[i]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of     0 : 100 %\n",
            "Accuracy of     1 : 97 %\n",
            "Accuracy of     2 : 95 %\n",
            "Accuracy of     3 : 98 %\n",
            "Accuracy of     4 : 97 %\n",
            "Accuracy of     5 : 95 %\n",
            "Accuracy of     6 : 96 %\n",
            "Accuracy of     7 : 98 %\n",
            "Accuracy of     8 : 100 %\n",
            "Accuracy of     9 : 98 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45e9n8DIDdIL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}